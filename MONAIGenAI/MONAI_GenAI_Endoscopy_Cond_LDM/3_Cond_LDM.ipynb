{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Third Step: Conditioning LDM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9qtvMJeyDEd"
   },
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buPWzfNktPcs"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from monai import transforms\n",
    "from monai.apps import MedNISTDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import CacheDataset, DataLoader\n",
    "from monai.utils import first, set_determinism\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "\n",
    "from generative.inferers import LatentDiffusionInferer\n",
    "from generative.losses import PatchAdversarialLoss, PerceptualLoss\n",
    "from generative.networks import nets\n",
    "from generative.networks.schedulers import DDPMScheduler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import generative\n",
    "generative.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJAPdxNGyPVm"
   },
   "source": [
    "## Prepare Kvasir-SEG Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Bd5Fc3URcgb",
    "outputId": "dc93ca60-f8a5-44c2-98a8-edc6e7812743"
   },
   "outputs": [],
   "source": [
    "path_img = './Kvasir-SEG/images/'\n",
    "path_msk = './Kvasir-SEG/masks/'\n",
    "fnames_img = [f for f in os.listdir(path_img) if '.jpg' in f]\n",
    "datalist = []\n",
    "for fname in fnames_img:\n",
    "    data = {'image': path_img+fname, 'seg': path_msk+fname}\n",
    "    datalist.append(data)\n",
    "\n",
    "# Shuffle\n",
    "# np.random.shuffle(datalist)\n",
    "# Split the datalist to train and validation\n",
    "train_datalist = datalist[:950]\n",
    "val_datalist = datalist[950:]\n",
    "datalist[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioning LDM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MeoZD-HvyhM_"
   },
   "source": [
    "### Transforms and Dataloader Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eVbqfQMIveQS",
    "outputId": "2321a0c9-6d07-425c-b2bf-a924a7ceb7ce"
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "target_key = ['image', 'seg']\n",
    "shape = [128, 128]\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=target_key),\n",
    "        transforms.EnsureChannelFirstd(keys=target_key),\n",
    "        transforms.Resized(keys=target_key, spatial_size=shape),\n",
    "        transforms.ScaleIntensityRanged(keys=target_key,\n",
    "                                        a_min=0.0, a_max=255.0,\n",
    "                                        b_min=0.0, b_max=1.0, clip=True),\n",
    "        transforms.RandAffined(\n",
    "            keys=target_key,\n",
    "            rotate_range=[(-np.pi / 36, np.pi / 36), (-np.pi / 36, np.pi / 36)],\n",
    "            scale_range=[(-0.05, 0.05), (-0.05, 0.05)],\n",
    "            padding_mode=\"zeros\",\n",
    "            prob=0.5,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "train_ds = CacheDataset(data=train_datalist, transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4, persistent_workers=True)\n",
    "\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=target_key),\n",
    "        transforms.EnsureChannelFirstd(keys=target_key),\n",
    "        transforms.Resized(keys=target_key, spatial_size=shape),\n",
    "        transforms.ScaleIntensityRanged(keys=target_key,\n",
    "                                        a_min=0.0, a_max=255.0,\n",
    "                                        b_min=0.0, b_max=1.0, clip=True),\n",
    "    ]\n",
    ")\n",
    "val_ds = CacheDataset(data=val_datalist, transform=val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=4, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = first(train_loader)\n",
    "d['image'].shape, d['seg'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-trained AutoEncoderKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_img = nets.AutoencoderKL(\n",
    "    spatial_dims=2,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    num_channels=(32, 64, 64),\n",
    "    latent_channels=3,\n",
    "    num_res_blocks=1,\n",
    "    norm_num_groups=16,\n",
    "    attention_levels=(False, False, True),\n",
    ").to(device)\n",
    "\n",
    "state_dict_img = torch.load(os.path.join('models', 'AE_img.pt'))\n",
    "ae_img.load_state_dict(state_dict_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_mask = nets.AutoencoderKL(\n",
    "    spatial_dims=2,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    num_channels=(32, 64, 64),\n",
    "    latent_channels=32,\n",
    "    num_res_blocks=1,\n",
    "    norm_num_groups=16,\n",
    "    attention_levels=(False, False, True),\n",
    ").to(device)\n",
    "\n",
    "state_dict_mask = torch.load(os.path.join('models', 'AE_mask.pt'))\n",
    "ae_mask.load_state_dict(state_dict_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUfF4UXB5qa9"
   },
   "source": [
    "### Define LDM model, scaling factor, scheduler, inferer and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhVE8vDOysDj"
   },
   "outputs": [],
   "source": [
    "ldm = nets.DiffusionModelUNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    num_res_blocks=1,\n",
    "    num_channels=(32, 64, 64),\n",
    "    attention_levels=(False, True, True),\n",
    "    num_head_channels=(0, 64, 64),\n",
    "    with_conditioning=True,\n",
    "    cross_attention_dim=32\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling factor\n",
    "As mentioned in Rombach et al. [1] Section 4.3.2 and D.1, the signal-to-noise ratio (induced by the scale of the latent space) can affect the results obtained with the LDM, if the standard deviation of the latent space distribution drifts too much from that of a Gaussian. For this reason, it is best practice to use a scaling factor to adapt this standard deviation.\n",
    "\n",
    "Note: In case where the latent space is close to a Gaussian distribution, the scaling factor will be close to one, and the results will not differ from those obtained when it is not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    with autocast(enabled=True):\n",
    "        z = ae_img.encode_stage_2_inputs(d['image'].to(device))\n",
    "\n",
    "print(f\"Scaling factor set to {1/torch.std(z)}\")\n",
    "scale_factor = 1 / torch.std(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_timesteps = 1000\n",
    "scheduler = DDPMScheduler(num_train_timesteps=num_train_timesteps, schedule=\"scaled_linear_beta\", beta_start=0.0015, beta_end=0.0195)\n",
    "inferer = LatentDiffusionInferer(scheduler, scale_factor=scale_factor)\n",
    "optimizer_diff = torch.optim.Adam(params=ldm.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEP39VBI6sTv"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 642
    },
    "id": "YE0sRwUq5f5g",
    "outputId": "0d26912d-7d9e-4f1d-f3e8-b873fd4e9540"
   },
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "val_interval = 1\n",
    "epoch_loss_list = []\n",
    "val_epoch_loss_list = []\n",
    "ae_img.eval()\n",
    "ae_mask.eval()\n",
    "scaler = GradScaler()\n",
    "\n",
    "first_batch = first(train_loader)\n",
    "z = ae_img.encode_stage_2_inputs(d['image'].to(device))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    ldm.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=70)\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    # Training\n",
    "    for step, batch in progress_bar:\n",
    "        images = batch['image'].to(device)\n",
    "        masks = batch['seg'].to(device)\n",
    "        optimizer_diff.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(enabled=True):\n",
    "            # Generate random noise\n",
    "            noise = torch.randn_like(z).to(device)\n",
    "            \n",
    "            # Get encoded condition\n",
    "            condition = ae_mask.encode_stage_2_inputs(masks).mean(dim=3).transpose(1, 2).to(device)\n",
    "            \n",
    "            # Create timesteps\n",
    "            timesteps = torch.randint(\n",
    "                0, inferer.scheduler.num_train_timesteps, (images.shape[0],), device=images.device\n",
    "            ).long()\n",
    "\n",
    "            # Get model prediction\n",
    "            noise_pred = inferer(\n",
    "                inputs=images, autoencoder_model=ae_img, diffusion_model=ldm, noise=noise, timesteps=timesteps, condition=condition\n",
    "            )\n",
    "\n",
    "            loss = F.mse_loss(noise_pred.float(), noise.float())\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer_diff)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        progress_bar.set_postfix({\"loss\": epoch_loss / (step + 1)})\n",
    "        break\n",
    "    epoch_loss_list.append(epoch_loss / (step + 1))\n",
    "    # Validation\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        ldm.eval()\n",
    "        val_epoch_loss = 0\n",
    "        for step, batch in enumerate(val_loader):\n",
    "            images = batch['image'].to(device)\n",
    "            masks = batch['seg'].to(device)\n",
    "            with torch.no_grad():\n",
    "                with autocast(enabled=True):\n",
    "                    noise = torch.randn_like(z).to(device)\n",
    "                    condition = ae_mask.encode_stage_2_inputs(masks).mean(dim=3).transpose(1, 2).to(device)\n",
    "                    timesteps = torch.randint(\n",
    "                        0, inferer.scheduler.num_train_timesteps, (images.shape[0],), device=images.device\n",
    "                    ).long()\n",
    "\n",
    "                    noise_pred = inferer(\n",
    "                        inputs=images, autoencoder_model=ae_img, diffusion_model=ldm, noise=noise, timesteps=timesteps, condition=condition\n",
    "                    )\n",
    "                    \n",
    "                    val_loss = F.mse_loss(noise_pred.float(), noise.float())\n",
    "\n",
    "            val_epoch_loss += val_loss.item()\n",
    "        val_epoch_loss_list.append(val_epoch_loss / (step + 1))\n",
    "        print({\"val_loss\": val_epoch_loss / (step + 1)})\n",
    "        # Sampling image during training\n",
    "        noise = torch.randn((1,)+tuple(images[0].shape))\n",
    "        noise = noise.to(device)\n",
    "        scheduler.set_timesteps(num_inference_steps=num_train_timesteps)\n",
    "        with autocast(enabled=True):\n",
    "            generated, intermediates = inferer.sample(\n",
    "            input_noise=noise,\n",
    "            diffusion_model=ldm,\n",
    "            autoencoder_model=ae_img,\n",
    "            scheduler=scheduler,\n",
    "            save_intermediates=True,\n",
    "            intermediate_steps=num_train_timesteps//10,\n",
    "            conditioning=condition\n",
    "            )\n",
    "        \n",
    "        plt.figure(figsize=(4, 12))\n",
    "        plt.subplot(131)\n",
    "        plt.imshow(images[0].cpu().numpy().transpose([2,1,0]), vmin=0, vmax=1)\n",
    "        plt.tight_layout()\n",
    "        plt.axis(\"off\")\n",
    "        plt.title('Original')\n",
    "        plt.subplot(132)\n",
    "        plt.imshow(masks[0].cpu().numpy().transpose([2,1,0]), vmin=0, vmax=1)\n",
    "        plt.tight_layout()\n",
    "        plt.axis(\"off\")\n",
    "        plt.title('Conditioning')\n",
    "        plt.subplot(133)\n",
    "        plt.imshow(generated[0].float().cpu().numpy().transpose([2,1,0]), vmin=0, vmax=1)\n",
    "        plt.tight_layout()\n",
    "        plt.axis(\"off\")\n",
    "        plt.title('Generated')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_loss_list)\n",
    "plt.title(\"Learning Curves\", fontsize=20)\n",
    "plt.plot(epoch_loss_list)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.xlabel(\"Epochs\", fontsize=16)\n",
    "plt.ylabel(\"Loss\", fontsize=16)\n",
    "plt.legend(prop={\"size\": 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Adversarial Training Curves\", fontsize=20)\n",
    "plt.plot(epoch_gen_loss_list, color=\"C0\", linewidth=2.0, label=\"Generator\")\n",
    "plt.plot(epoch_disc_loss_list, color=\"C1\", linewidth=2.0, label=\"Discriminator\")\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.xlabel(\"Epochs\", fontsize=16)\n",
    "plt.ylabel(\"Loss\", fontsize=16)\n",
    "plt.legend(prop={\"size\": 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis():\n",
    "    ldm.eval()\n",
    "    batch = first(val_loader)\n",
    "    origin = batch['image'].to(device)\n",
    "    masks = batch['seg'].to(device)\n",
    "    condition = ae_mask.encode_stage_2_inputs(masks).mean(dim=3).transpose(1, 2).to(device)\n",
    "    noise = torch.randn_like(z)\n",
    "    noise = noise.to(device)\n",
    "    scheduler.set_timesteps(num_inference_steps=num_train_timesteps)\n",
    "    with autocast(enabled=True):\n",
    "        image, intermediates = inferer.sample(\n",
    "            input_noise=noise,\n",
    "            diffusion_model=ldm,\n",
    "            autoencoder_model=ae_img,\n",
    "            scheduler=scheduler,\n",
    "            save_intermediates=True,\n",
    "            intermediate_steps=num_train_timesteps//10,\n",
    "            conditioning=condition\n",
    "        )\n",
    "\n",
    "    chain = torch.cat(intermediates, dim=-1)\n",
    "\n",
    "    plt.figure(figsize=(3, 6))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(images[0].cpu().numpy().transpose([2,1,0]), vmin=0, vmax=1)\n",
    "    plt.tight_layout()\n",
    "    plt.axis(\"off\")\n",
    "    plt.title('Original')\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(masks[0].cpu().numpy().transpose([2,1,0]), vmin=0, vmax=1)\n",
    "    plt.tight_layout()\n",
    "    plt.axis(\"off\")\n",
    "    plt.title('Conditioning')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10, 50))\n",
    "    plt.style.use(\"default\")\n",
    "    plt.imshow(chain[0].float().cpu().numpy().transpose([1,2,0]), vmin=0, vmax=1)\n",
    "    plt.tight_layout()\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    return chain\n",
    "\n",
    "_ = vis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
