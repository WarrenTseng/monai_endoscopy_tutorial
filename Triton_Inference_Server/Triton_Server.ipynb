{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5164b8b0",
   "metadata": {},
   "source": [
    "# Triton Inference Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc6de27",
   "metadata": {},
   "source": [
    "### Model Management - Examples\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9f087c",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5c20ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree examples/chestxray_cls/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045935a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat examples/chestxray_cls/config.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ef5717",
   "metadata": {},
   "source": [
    "---\n",
    "In the config above, basically we have to define:\n",
    "- Model name\n",
    "- Platform: TensorRT (tensorrt_plan), PyTorch (pytorch_libtorch), Tensorflow (tensorflow_graphdef) or others\n",
    "- Max batch size\n",
    "- Input description includes input node name, data type, array format and array dimensions\n",
    "- Output description includes output node name, data type and array dimensions\n",
    "- Instance group: Define the number of model instance you want to serve in the GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d12f802",
   "metadata": {},
   "source": [
    "#### Segmentation - TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985538fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree examples/covid19_seg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7909a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat examples/covid19_seg/config.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f39c6e0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc5d9d3",
   "metadata": {},
   "source": [
    "## Manage the Colonoscopy Segmentation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169ce956",
   "metadata": {},
   "source": [
    "Create folders for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d74d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./triton_models/endo_seg\n",
    "!mkdir -p ./triton_models/endo_seg/1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14123167",
   "metadata": {},
   "source": [
    "Copy our TRT model into the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01596189",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ../TensorRT/model_fp16.engine ./triton_models/endo_seg/1/model.plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5550ba",
   "metadata": {},
   "source": [
    "Get the input and output node names and shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b120154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "EXPLICIT_BATCH = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "\n",
    "builder = trt.Builder(TRT_LOGGER)\n",
    "network = builder.create_network(EXPLICIT_BATCH)\n",
    "parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "with open('../MONAICore/model.onnx', 'rb') as model:\n",
    "    parser.parse(model.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f3b3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = network.get_input(0)\n",
    "inputs.name, inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e8d69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = network.get_output(0)\n",
    "outputs.name, outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799dfae4",
   "metadata": {},
   "source": [
    "Generate the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6b4b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile triton_models/endo_seg/config.pbtxt\n",
    "name: \"endo_seg\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    "    {\n",
    "      name: \"input.1\"\n",
    "      data_type: TYPE_FP32\n",
    "      dims: [ 3, 256, 256 ]\n",
    "    }\n",
    "]\n",
    "output [\n",
    "    {\n",
    "      name: \"495\"\n",
    "      data_type: TYPE_FP32\n",
    "      dims: [ 1, 256, 256 ]\n",
    "    }\n",
    "]\n",
    "instance_group [\n",
    "    {\n",
    "      kind: KIND_GPU\n",
    "      count: 1\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013ac6db",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac4cce2",
   "metadata": {},
   "source": [
    "## Run Triton Inference Server\n",
    "Run below command in Triton Inference Server container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d236d7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tritonserver --model-store=/mount/src/Triton_Inference_Server/triton_models/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670e3059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
